# WebDataScraper ğŸ•¸ï¸

## Overview

WebDataScraper is a configurable Python web scraper designed for portfolio and real-world automation tasks.  
It can extract structured data from websites using CSS selectors, follow pagination links, and export results to JSON and CSV.

This project demonstrates:

- Python scripting and automation  
- Web scraping with `requests` and `BeautifulSoup`  
- Config-driven architecture (no hard-coded site logic)  
- Clean code structure and CLI interface  
- Data export to JSON and CSV  

> **Important:** Always respect website terms of service and robots.txt.  
> The included example uses [quotes.toscrape.com](https://quotes.toscrape.com/), a site created specifically for scraping practice.

---

## Features

- ğŸ§© Config-based scraping (no code changes needed for new sites)  
- ğŸ“„ CSS selector support for flexible extraction  
- ğŸ“‘ Pagination via "next page" links  
- ğŸ•’ Optional delay between requests (be a good net citizen)  
- ğŸ’¾ Export to both JSON and CSV  
- ğŸ–¥ï¸ Simple CLI with `--config` and `--output-base` options  

---

## Installation

```bash
git clone https://github.com/SANTIK-Genius/WebDataScraper.git
cd WebDataScraper

pip install -r requirements.txt
```

---

## Usage

```bash
python scraper.py --config config_example.json --output-base output/quotes
```

This will:

- Fetch several pages from `quotes.toscrape.com`  
- Extract quote text, author and tags  
- Save:  
  - `output/quotes.json`  
  - `output/quotes.csv`  

---

## Configuration

The scraper is powered by a JSON config file. Example:

```json
{
  "start_url": "https://quotes.toscrape.com/",
  "item_selector": "div.quote",
  "fields": {
    "text": { "selector": "span.text" },
    "author": { "selector": "small.author" },
    "tags": { "selector": "div.tags a.tag", "multiple": true }
  },
  "pagination": {
    "next_page_selector": "li.next a",
    "max_pages": 5
  },
  "delay_seconds": 1.0
}
```

- `start_url` â€“ first page to scrape  
- `item_selector` â€“ CSS selector for each item block  
- `fields` â€“ which data to extract from each item  
- `pagination.next_page_selector` â€“ selector for the â€œnext pageâ€ link  
- `pagination.max_pages` â€“ safety limit for the number of pages  
- `delay_seconds` â€“ delay between requests in seconds  

---

## Project Structure

```text
WebDataScraper/
â”œâ”€â”€ scraper.py              # main scraper script
â”œâ”€â”€ config_example.json     # example configuration
â”œâ”€â”€ requirements.txt        # dependencies
â””â”€â”€ README.md               # project documentation
```

---

## License

MIT (or your preferred license)
